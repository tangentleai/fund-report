#!/usr/bin/env python3
"""
åŸºé‡‘å­£æŠ¥è§£æ Demo
æå–åŸºé‡‘ç»ç†è§‚ç‚¹å¹¶è½¬æ¢ä¸ºæ’­å®¢è„šæœ¬
"""

import re
import json
import requests
from pathlib import Path
from typing import Optional, Dict, List, Any

# æµ‹è¯•ç”¨åŸºé‡‘åˆ—è¡¨
TEST_FUNDS = [
    {"code": "005827", "name": "æ˜“æ–¹è¾¾è“ç­¹ç²¾é€‰æ··åˆ", "manager": "å¼ å¤"},
    {"code": "003095", "name": "ä¸­æ¬§åŒ»ç–—å¥åº·æ··åˆA", "manager": "è‘›å…°"},
    {"code": "161725", "name": "æ‹›å•†ä¸­è¯ç™½é…’æŒ‡æ•°", "manager": "ä¾¯æ˜Š"},
]


def download_fund_report(fund_code: str, fund_name: str) -> Optional[str]:
    """
    ä¸‹è½½åŸºé‡‘å­£æŠ¥PDF
    æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®ï¼Œå®é™…éœ€è¦ä»AKShareæˆ–å¤©å¤©åŸºé‡‘è·å–çœŸå®PDFé“¾æ¥
    """
    print(f"\nğŸ“¥ æ­£åœ¨è·å– {fund_name}({fund_code}) çš„å­£æŠ¥...")

    # æ¨¡æ‹ŸPDFä¸‹è½½ï¼ˆå®é™…é¡¹ç›®ä¸­ä½¿ç”¨AKShareè·å–çœŸå®é“¾æ¥ï¼‰
    # è¿™é‡Œè¿”å›ç¤ºä¾‹æ–‡æœ¬ç”¨äºæµ‹è¯•è§£æé€»è¾‘
    return None


def parse_pdf_content(pdf_text: str) -> Dict[str, Any]:
    """
    è§£æPDFå†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯
    """
    result: Dict[str, Any] = {
        "fund_name": None,
        "report_date": None,
        "manager_viewpoint": None,
        "market_analysis": None,
        "future_outlook": None,
    }

    # 1. æå–åŸºé‡‘åç§°
    name_pattern = r"åŸºé‡‘ç®€ç§°[ï¼š:]\s*([^\n]+)"
    match = re.search(name_pattern, pdf_text)
    if match:
        result["fund_name"] = match.group(1).strip()

    # 2. æå–æŠ¥å‘Šæ—¥æœŸ
    date_patterns = [
        r"æŠ¥å‘ŠæœŸ[ï¼š:]\s*(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)",
        r"(\d{4})å¹´ç¬¬([ä¸€äºŒä¸‰å››])å­£åº¦æŠ¥å‘Š",
        r"æˆªè‡³\s*(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)",
    ]
    for pattern in date_patterns:
        match = re.search(pattern, pdf_text)
        if match:
            result["report_date"] = match.group(0)
            break

    # 3. æå–åŸºé‡‘ç»ç†è§‚ç‚¹ - æ ¸å¿ƒé€»è¾‘
    viewpoint = extract_manager_viewpoint(pdf_text)
    result["manager_viewpoint"] = viewpoint

    return result


def extract_manager_viewpoint(text: str) -> Optional[str]:
    """
    æå–åŸºé‡‘ç»ç†è§‚ç‚¹ç« èŠ‚
    å­£æŠ¥ç»“æ„ç›¸å¯¹å›ºå®šï¼Œé‡ç‚¹æå–"æŠ•èµ„ç­–ç•¥å’Œè¿ä½œåˆ†æ"éƒ¨åˆ†
    """
    # æ¸…ç†æ–‡æœ¬
    text = clean_text(text)

    # æ›´æ™ºèƒ½çš„ç›®å½•æ£€æµ‹å’Œè·³è¿‡
    # æˆ‘ä»¬æ‰¾Â§4ï¼Œä½†ç›®å½•ä¹Ÿæœ‰Â§4ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°çœŸæ­£çš„æ­£æ–‡å¼€å§‹
    # ç­–ç•¥ï¼šæˆ‘ä»¬å…ˆæ‰¾ç¬¬ä¸€ä¸ª"4.2"æˆ–"4.3"æˆ–"4.4"åé¢è·Ÿç€ä¸€æ®µå¾ˆé•¿æ–‡æœ¬çš„ï¼Œé‚£å°±æ˜¯æ­£æ–‡
    
    # æˆ‘ä»¬å…ˆå°è¯•ç›´æ¥æ‰¾ï¼Œä¸è·³è¿‡Â§4ï¼Œè®©åŒ¹é…æ¨¡å¼æ›´å¼ºå¤§ï¼Œå¹¶ä¸”è®©åŒ¹é…æ›´å‡†ç¡®
    # ç„¶åï¼Œæˆ‘ä»¬åœ¨åŒ¹é…åˆ°ç»“æœåï¼ŒéªŒè¯å†…å®¹æ˜¯ä¸æ˜¯æ­£æ–‡ï¼ˆä¸æ˜¯åªæœ‰å‡ è¡Œæˆ–æœ‰ç‚¹ï¼‰
    # æˆ‘ä»¬ä¹Ÿå¯ä»¥å°è¯•å¤šç§åŒ¹é…æ–¹æ³•ï¼Œå¹¶ä¸”å¦‚æœç¬¬ä¸€æ¬¡åŒ¹é…åˆ°çš„å†…å®¹å¤ªçŸ­ï¼ˆæ¯”å¦‚<100å­—ç¬¦ï¼‰ï¼Œæˆ‘ä»¬ç»§ç»­æ‰¾
    # æˆ–è€…ï¼Œæˆ‘ä»¬æ‰¾æŠ•èµ„ç­–ç•¥å’Œè¿ä½œåˆ†æçš„æ‰€æœ‰å‡ºç°ï¼Œç„¶åé€‰å†…å®¹æœ€é•¿çš„ä¸€ä¸ªï¼
    
    # æˆ‘ä»¬å…ˆç›´æ¥å°è¯•æˆ‘ä»¬çš„åŒ¹é…æ¨¡å¼
    # ä¸å…ˆå¤„ç†æ–‡æœ¬ï¼Œå…ˆè®©åŸæ–‡æœ¬å®Œæ•´

    # æ”¶é›†æ‰€æœ‰å¯èƒ½çš„å€™é€‰è§‚ç‚¹
    candidates = []
    
    # å¤šç§åŒ¹é…æ¨¡å¼ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
    patterns = [
        # æ¨¡å¼1: ç²¾ç¡®åŒ¹é…"4.XæŠ¥å‘ŠæœŸå†…åŸºé‡‘æŠ•èµ„ç­–ç•¥å’Œè¿ä½œåˆ†æ"ï¼ˆç‚¹åæ— ç©ºæ ¼ï¼‰
        r"4\.\d+æŠ¥å‘ŠæœŸå†…åŸºé‡‘çš„æŠ•èµ„ç­–ç•¥å’Œè¿ä½œåˆ†æ\s*[ï¼š:]?\s*([\s\S]{20,8000}?)(?=\s*(?:4\.\d+|Â§|ç¬¬[äº”å…­ä¸ƒå…«]èŠ‚|ç¬¬äº”èŠ‚|é‡è¦æç¤º|æŠ•èµ„ç»„åˆæŠ¥å‘Š|æŠ¥å‘ŠæœŸå†…åŸºé‡‘çš„ä¸šç»©è¡¨ç°|åŸºé‡‘çš„ä¸šç»©è¡¨ç°|åŸºé‡‘æŒæœ‰äººæ•°|åŸºé‡‘èµ„äº§å‡€å€¼é¢„è­¦|é‡å¤§äº‹é¡¹æç¤º|è´¢åŠ¡æŒ‡æ ‡))",
        # æ¨¡å¼2: ç²¾ç¡®åŒ¹é…"æŠ¥å‘ŠæœŸå†…åŸºé‡‘æŠ•èµ„ç­–ç•¥å’Œè¿ä½œåˆ†æ"
        r"æŠ¥å‘ŠæœŸå†…åŸºé‡‘æŠ•èµ„ç­–ç•¥å’Œè¿ä½œåˆ†æ\s*[ï¼š:]?\s*\n?\s*([^Â§]+?)(?=\s*(?:Â§|ç¬¬[äº”å…­ä¸ƒå…«]èŠ‚|ç¬¬äº”èŠ‚|é‡è¦æç¤º|æŠ•èµ„ç»„åˆæŠ¥å‘Š|æŠ¥å‘ŠæœŸå†…åŸºé‡‘çš„ä¸šç»©è¡¨ç°|åŸºé‡‘çš„ä¸šç»©è¡¨ç°|åŸºé‡‘æŒæœ‰äººæ•°|åŸºé‡‘èµ„äº§å‡€å€¼é¢„è­¦|é‡å¤§äº‹é¡¹æç¤º|è´¢åŠ¡æŒ‡æ ‡))",
        # æ¨¡å¼3: åŒ¹é…"æŠ•èµ„ç­–ç•¥å’Œè¿ä½œåˆ†æ"é•¿æ®µè½ï¼ˆé™ä½é•¿åº¦è¦æ±‚ä»200é™åˆ°20ï¼‰
        r"æŠ•èµ„ç­–ç•¥å’Œè¿ä½œåˆ†æ\s*[ï¼š:]?\s*([\s\S]{20,8000}?)(?=\s*(?:Â§|ç¬¬[äº”å…­ä¸ƒå…«]èŠ‚|ç¬¬äº”èŠ‚|é‡è¦æç¤º|æŠ•èµ„ç»„åˆæŠ¥å‘Š|æŠ¥å‘ŠæœŸå†…åŸºé‡‘çš„ä¸šç»©è¡¨ç°|åŸºé‡‘çš„ä¸šç»©è¡¨ç°|åŸºé‡‘æŒæœ‰äººæ•°|åŸºé‡‘èµ„äº§å‡€å€¼é¢„è­¦|é‡å¤§äº‹é¡¹æç¤º|è´¢åŠ¡æŒ‡æ ‡))",
        # æ¨¡å¼4: åŒ¹é…"4.XæŠ•èµ„ç­–ç•¥å’Œè¿ä½œåˆ†æ"ï¼ˆç‚¹åæ— ç©ºæ ¼ï¼‰
        r"4\.\d+æŠ•èµ„ç­–ç•¥å’Œè¿ä½œåˆ†æ\s*[ï¼š:]?\s*([\s\S]{20,8000}?)(?=\s*(?:4\.\d+|Â§|ç¬¬[äº”å…­ä¸ƒå…«]èŠ‚|ç¬¬äº”èŠ‚|é‡è¦æç¤º|æŠ•èµ„ç»„åˆæŠ¥å‘Š|æŠ¥å‘ŠæœŸå†…åŸºé‡‘çš„ä¸šç»©è¡¨ç°|åŸºé‡‘çš„ä¸šç»©è¡¨ç°|åŸºé‡‘æŒæœ‰äººæ•°|åŸºé‡‘èµ„äº§å‡€å€¼é¢„è­¦|é‡å¤§äº‹é¡¹æç¤º|è´¢åŠ¡æŒ‡æ ‡))",
        # æ¨¡å¼5: åŒ¹é…"4\. X"æ ¼å¼ï¼ˆç‚¹åæœ‰ç©ºæ ¼ï¼‰
        r"4\.\s*\d+\s*æŠ¥å‘ŠæœŸå†…åŸºé‡‘çš„æŠ•èµ„ç­–ç•¥å’Œè¿ä½œåˆ†æ\s*[ï¼š:]?\s*([\s\S]{20,8000}?)(?=\s*(?:4\.\s*\d+|Â§|ç¬¬[äº”å…­ä¸ƒå…«]èŠ‚|ç¬¬äº”èŠ‚|é‡è¦æç¤º|æŠ•èµ„ç»„åˆæŠ¥å‘Š|æŠ¥å‘ŠæœŸå†…åŸºé‡‘çš„ä¸šç»©è¡¨ç°|åŸºé‡‘çš„ä¸šç»©è¡¨ç°|åŸºé‡‘æŒæœ‰äººæ•°|åŸºé‡‘èµ„äº§å‡€å€¼é¢„è­¦|é‡å¤§äº‹é¡¹æç¤º|è´¢åŠ¡æŒ‡æ ‡))",
    ]

    # å°è¯•æ‰€æœ‰æ¨¡å¼ï¼Œæ”¶é›†æ‰€æœ‰å€™é€‰
    for pattern in patterns:
        matches = list(re.finditer(pattern, text, re.DOTALL | re.IGNORECASE))
        for match in matches:
            viewpoint = match.group(1).strip()
            # è¿›ä¸€æ­¥æ¸…æ´—
            viewpoint = post_clean(viewpoint)
            if validate_viewpoint(viewpoint):
                candidates.append(viewpoint)
    
    # å¦‚æœæœ‰å€™é€‰ï¼Œé€‰æœ€é•¿çš„é‚£ä¸ª
    if candidates:
        candidates.sort(key=lambda x: len(x), reverse=True)
        return candidates[0]
    
    # åªæœ‰åœ¨æ²¡æœ‰å…¶ä»–å€™é€‰çš„æƒ…å†µä¸‹ï¼Œæ‰å°è¯•å…œåº•æ–¹æ¡ˆ
    fallback = fallback_extract(text)
    if fallback:
        fallback = post_clean(fallback)
        if validate_viewpoint(fallback):
            return fallback
    
    # æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å€™é€‰
    return None


def clean_text(text: str) -> str:
    """æ–‡æœ¬é¢„å¤„ç†"""
    # ç»Ÿä¸€æ¢è¡Œç¬¦
    text = text.replace("\r\n", "\n").replace("\r", "\n")

    # å»æ‰é¡µçœ‰é¡µè„š (å¦‚ "ç¬¬ X é¡µ å…± Y é¡µ")
    text = re.sub(r"ç¬¬\s*\d+\s*é¡µ\s*å…±\s*\d+\s*é¡µ", "", text)
    text = re.sub(r"Page\s*\d+\s*of\s*\d+", "", text, flags=re.IGNORECASE)

    # å»æ‰é¡µç  (å¦‚ "- 3 -" æˆ– "â€”3â€”")
    text = re.sub(r"[\-â€“â€”]\s*\d+\s*[\-â€“â€”]", "", text)

    # åˆå¹¶å•è¡Œæ¢è¡Œï¼Œä¿ç•™æ®µè½æ¢è¡Œ
    text = re.sub(r"(?<!\n)\n(?!\n)", " ", text)
    text = re.sub(r" +", " ", text)

    # åˆå¹¶å¤šä¸ªæ¢è¡Œ
    text = re.sub(r"\n{3,}", "\n\n", text)

    return text.strip()


def post_clean(text: str) -> str:
    """è§‚ç‚¹æå–åæ¸…æ´—"""
    # å»æ‰å¸¸è§çš„åºŸè¯å¼€å¤´
    useless_prefixes = [
        "æŠ¥å‘ŠæœŸå†…ï¼Œ",
        "æœ¬æŠ¥å‘ŠæœŸå†…ï¼Œ",
        "æœ¬åŸºé‡‘",
        "2024å¹´",
    ]
    for prefix in useless_prefixes:
        if text.startswith(prefix):
            text = text[len(prefix) :].strip()

    # å»æ‰è¡¨æ ¼æ®‹ç•™
    text = re.sub(r"\|\s*[^\|]+\s*\|", "", text)

    # å»æ‰åŸºé‡‘ç»ç†ç®€ä»‹ç­‰å™ªéŸ³
    noise_patterns = [
        r"å§“å\s+\w+\s+èŒåŠ¡\s+åŸºé‡‘ç»ç†",
        r"4\.\d+\s*åŸºé‡‘ç»ç†.*?ç®€ä»‹",
        r"4\.\d+\s*ç®¡ç†äººå¯¹æŠ¥å‘ŠæœŸå†….*?è¯´æ˜",
        r"æŠ•èµ„ç­–ç•¥å’Œè¿ä½œåˆ†æ\s*[ï¼š:]?\s*\n?",
        r"[\u4e00-\u9fffA-Za-z0-9ï¼ˆï¼‰()Â·\-]{6,}è¯åˆ¸æŠ•èµ„åŸºé‡‘\d{4}å¹´ç¬¬[ä¸€äºŒä¸‰å››1234]å­£åº¦æŠ¥å‘Š",
        r"[\u4e00-\u9fffA-Za-z0-9ï¼ˆï¼‰()Â·\-]{6,}è¯åˆ¸æŠ•èµ„åŸºé‡‘\d{4}å¹´ä¸­æœŸæŠ¥å‘Š",
        r"[\u4e00-\u9fffA-Za-z0-9ï¼ˆï¼‰()Â·\-]{6,}è¯åˆ¸æŠ•èµ„åŸºé‡‘\d{4}å¹´åº¦æŠ¥å‘Š",
    ]
    for pattern in noise_patterns:
        text = re.sub(pattern, "", text, flags=re.IGNORECASE)

    stop_pattern = (
        r"(æŠ¥å‘ŠæœŸå†…åŸºé‡‘çš„ä¸šç»©è¡¨ç°|åŸºé‡‘çš„ä¸šç»©è¡¨ç°|åŸºé‡‘æŒæœ‰äººæ•°|åŸºé‡‘èµ„äº§å‡€å€¼é¢„è­¦"
        r"|é‡å¤§äº‹é¡¹æç¤º|è´¢åŠ¡æŒ‡æ ‡|æŠ•èµ„ç»„åˆæŠ¥å‘Š|è´¢åŠ¡ä¼šè®¡æŠ¥å‘Š)"
    )
    text = re.split(stop_pattern, text, maxsplit=1)[0]
    text = re.sub(r"(4\.\d+|ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«]èŠ‚)\s*$", "", text).strip()

    # æ¸…ç†è¡Œ
    lines = text.split("\n")
    cleaned_lines = []
    for line in lines:
        line = line.strip()
        # è¿‡æ»¤æ‰å¤ªçŸ­çš„è¡Œ
        if len(line) < 3:
            continue
        # è¿‡æ»¤æ‰åªæœ‰æ•°å­—æˆ–æ ‡ç‚¹çš„è¡Œ
        if re.match(r"^[\d\s\.\-â€”]+$", line):
            continue
        # è¿‡æ»¤æ‰åŸºé‡‘ç»ç†ä¿¡æ¯è¡Œ
        if re.match(r"^å§“å|èŒåŠ¡|åŸºé‡‘ç»ç†", line):
            continue
        # ä¿ç•™æœ‰å®è´¨å†…å®¹çš„è¡Œ
        if len(line) > 10 or re.search(r"[\u4e00-\u9fff]{3,}", line):
            cleaned_lines.append(line)

    result = "\n".join(cleaned_lines).strip()

    # æœ€ç»ˆæ¸…ç†
    result = re.sub(r"\n{3,}", "\n\n", result)

    return result


def validate_viewpoint(text: str) -> bool:
    """éªŒè¯æå–çš„è§‚ç‚¹æ˜¯å¦æœ‰æ•ˆ"""
    if not text or len(text) < 20:
        return False
    
    # åªè¦æœ‰å†…å®¹å°±è¿”å›ï¼Œä¸å¼ºåˆ¶è¦æ±‚å…³é”®è¯æ•°é‡
    # è¿™æ ·å¯ä»¥æ”¯æŒç®€çŸ­çš„è§‚ç‚¹
    return True


def fallback_extract(text: str) -> Optional[str]:
    """å…œåº•æå–æ–¹æ¡ˆ"""
    # æŸ¥æ‰¾åŒ…å«æŠ•èµ„å…³é”®è¯çš„æœ€é•¿è¿ç»­æ®µè½
    paragraphs = re.split(r"\n{2,}", text)

    best_paragraph = None
    best_score = 0

    for para in paragraphs:
        if len(para) < 100 or len(para) > 5000:
            continue

        # è¯„åˆ†ï¼šåŒ…å«çš„æŠ•èµ„å…³é”®è¯è¶Šå¤šã€æ–‡æœ¬è¶Šé•¿ï¼Œåˆ†æ•°è¶Šé«˜
        score = len(para)
        investment_keywords = ["ç­–ç•¥", "æŠ•èµ„", "å¸‚åœº", "é…ç½®", "è¡Œä¸š", "é£é™©"]
        score += sum(50 for kw in investment_keywords if kw in para)

        if score > best_score:
            best_score = score
            best_paragraph = para

    return best_paragraph.strip() if best_paragraph else None


def generate_podcast_script(viewpoint: str, fund_name: str, manager: str) -> str:
    """
    å°†åŸºé‡‘ç»ç†è§‚ç‚¹è½¬æ¢ä¸ºæ’­å®¢å¯¹è¯è„šæœ¬
    """
    # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è°ƒç”¨AIæ¨¡å‹
    script = f"""
ã€æ’­å®¢è„šæœ¬ã€‘{fund_name} å­£æŠ¥è§£è¯»

ä¸»æŒäººå°æ˜ï¼šå¤§å®¶å¥½ï¼Œæ¬¢è¿æ”¶å¬æœ¬æœŸåŸºé‡‘å­£æŠ¥è§£è¯»ã€‚ä»Šå¤©æˆ‘ä»¬èŠçš„æ˜¯{fund_name}ï¼ŒåŸºé‡‘ç»ç†æ˜¯{manager}ã€‚

ä¸»æŒäººå°çº¢ï¼šå…ˆæ¥çœ‹åŸºé‡‘ç»ç†åœ¨æœ€æ–°å­£æŠ¥ä¸­çš„è§‚ç‚¹ï¼š

{viewpoint[:500]}...

ä¸»æŒäººå°æ˜ï¼šä»è¿™æ®µè¯å¯ä»¥çœ‹å‡ºï¼ŒåŸºé‡‘ç»ç†å¯¹åå¸‚çš„æ€åº¦...

ä¸»æŒäººå°çº¢ï¼šé‚£æˆ‘ä»¬å¯¹æ™®é€šæŠ•èµ„è€…æœ‰ä»€ä¹ˆå»ºè®®å‘¢ï¼Ÿ
...
"""
    return script


def test_with_sample():
    """ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•è§£æé€»è¾‘"""

    # æ¨¡æ‹Ÿå­£æŠ¥æ–‡æœ¬ï¼ˆå®é™…æ˜¯ä»PDFæå–çš„ï¼‰
    sample_report = """
æ˜“æ–¹è¾¾è“ç­¹ç²¾é€‰æ··åˆå‹è¯åˆ¸æŠ•èµ„åŸºé‡‘
2024å¹´ç¬¬4å­£åº¦æŠ¥å‘Š

Â§1 é‡è¦æç¤º
åŸºé‡‘ç®¡ç†äººçš„è‘£äº‹ä¼šåŠè‘£äº‹ä¿è¯æœ¬æŠ¥å‘Šæ‰€è½½èµ„æ–™ä¸å­˜åœ¨è™šå‡è®°è½½...

Â§4 ç®¡ç†äººæŠ¥å‘Š
4.1 åŸºé‡‘ç»ç†(æˆ–åŸºé‡‘ç»ç†å°ç»„)ç®€ä»‹
å§“å å¼ å¤ èŒåŠ¡ åŸºé‡‘ç»ç†

4.2 ç®¡ç†äººå¯¹æŠ¥å‘ŠæœŸå†…æœ¬åŸºé‡‘æŠ•èµ„ç­–ç•¥å’Œè¿ä½œåˆ†æçš„è¯´æ˜

æŠ¥å‘ŠæœŸå†…ï¼ŒAè‚¡å¸‚åœºå‘ˆç°éœ‡è¡èµ°åŠ¿ï¼Œæ²ªæ·±300æŒ‡æ•°ä¸Šæ¶¨...æœ¬åŸºé‡‘ä¿æŒè¾ƒé«˜çš„è‚¡ç¥¨ä»“ä½ï¼Œé‡ç‚¹é…ç½®äº†æ¶ˆè´¹ã€åŒ»è¯ç­‰è¡Œä¸šçš„ä¼˜è´¨ä¼ä¸šã€‚

ä»é•¿æœŸçœ‹ï¼Œä¸­å›½ç»æµçš„åŸºæœ¬é¢ä¾ç„¶ç¨³å›ºï¼Œä¼˜è´¨ä¼ä¸šçš„æŠ¤åŸæ²³ä»åœ¨åŠ æ·±ã€‚æˆ‘ä»¬è®¤ä¸ºå½“å‰å¸‚åœºçš„ä¼°å€¼æ°´å¹³å¤„äºå†å²è¾ƒä½ä½ç½®ï¼Œä¸ºé•¿æœŸæŠ•èµ„è€…æä¾›äº†è¾ƒå¥½çš„å¸ƒå±€æœºä¼šã€‚

åœ¨æ“ä½œä¸Šï¼Œæœ¬åŸºé‡‘ç»´æŒäº†å¯¹ä¼˜è´¨ä¼ä¸šçš„é•¿æœŸæŒæœ‰ï¼Œå¹¶æ ¹æ®åŸºæœ¬é¢å˜åŒ–è¿›è¡Œäº†é€‚åº¦è°ƒæ•´ã€‚

Â§5 æŠ•èµ„ç»„åˆæŠ¥å‘Š
5.1 æŠ¥å‘ŠæœŸæœ«åŸºé‡‘èµ„äº§ç»„åˆæƒ…å†µ
...
"""

    print("=" * 60)
    print("ğŸ§ª æµ‹è¯•åŸºé‡‘å­£æŠ¥è§£æ")
    print("=" * 60)

    result = parse_pdf_content(sample_report)

    print(f"\nâœ… è§£æç»“æœï¼š")
    print(f"   åŸºé‡‘åç§°: {result['fund_name']}")
    print(f"   æŠ¥å‘Šæ—¥æœŸ: {result['report_date']}")
    print(f"\nğŸ“ åŸºé‡‘ç»ç†è§‚ç‚¹:")
    print(
        f"   {result['manager_viewpoint'][:300]}..."
        if result["manager_viewpoint"]
        else "   æœªæå–åˆ°è§‚ç‚¹"
    )

    if result["manager_viewpoint"]:
        print(f"\nğŸ™ï¸  æ’­å®¢è„šæœ¬é¢„è§ˆ:")
        script = generate_podcast_script(
            result["manager_viewpoint"], "æ˜“æ–¹è¾¾è“ç­¹ç²¾é€‰æ··åˆ", "å¼ å¤"
        )
        print(script[:500] + "...")

    return result


def test_real_fund(fund_code: str, fund_name: str, manager: str):
    """
    æµ‹è¯•çœŸå®åŸºé‡‘æ•°æ®ï¼ˆéœ€è¦AKShareï¼‰
    """
    try:
        import akshare as ak

        print(f"\n{'=' * 60}")
        print(f"ğŸ” è·å–çœŸå®æ•°æ®: {fund_name}({fund_code})")
        print(f"{'=' * 60}")

        # è·å–åŸºé‡‘å…¬å‘Šåˆ—è¡¨
        try:
            announcement_df = ak.fund_announcement_personnel_em(symbol=fund_code)
            print(f"âœ… æˆåŠŸè·å–å…¬å‘Šåˆ—è¡¨ï¼Œå…± {len(announcement_df)} æ¡")
            print(f"\næœ€è¿‘å‡ æ¡å…¬å‘Šï¼š")
            print(announcement_df.head(3)[["åç§°", "å‘å¸ƒæ—¶é—´"]].to_string(index=False))
        except Exception as e:
            print(f"âš ï¸  è·å–å…¬å‘Šåˆ—è¡¨å¤±è´¥: {e}")
            return None

        # æ³¨æ„ï¼šå®é™…PDFä¸‹è½½éœ€è¦è§£æå…¬å‘Šé“¾æ¥
        print(f"\nğŸ’¡ æç¤º: å®é™…é¡¹ç›®ä¸­éœ€è¦ä»å…¬å‘Šé“¾æ¥ä¸‹è½½PDFå¹¶è§£æ")

    except ImportError:
        print("âŒ æœªå®‰è£… AKShareï¼Œè¯·è¿è¡Œ: pip install akshare")
        return None

    return None


if __name__ == "__main__":
    # 1. å…ˆç”¨æ¨¡æ‹Ÿæ•°æ®æµ‹è¯•è§£æé€»è¾‘
    print("\n" + "=" * 60)
    print("ç¬¬ä¸€æ­¥ï¼šæµ‹è¯•è§£æé€»è¾‘")
    print("=" * 60)
    test_with_sample()

    # 2. å°è¯•è·å–çœŸå®æ•°æ®ï¼ˆå¦‚æœå®‰è£…äº†AKShareï¼‰
    print("\n" + "=" * 60)
    print("ç¬¬äºŒæ­¥ï¼šæµ‹è¯•çœŸå®æ•°æ®è·å–")
    print("=" * 60)

    try:
        test_real_fund("005827", "æ˜“æ–¹è¾¾è“ç­¹ç²¾é€‰æ··åˆ", "å¼ å¤")
    except Exception as e:
        print(f"âš ï¸  çœŸå®æ•°æ®æµ‹è¯•è·³è¿‡: {e}")

    print("\n" + "=" * 60)
    print("âœ¨ Demo å®Œæˆï¼")
    print("=" * 60)
    print("\nä¸‹ä¸€æ­¥å»ºè®®ï¼š")
    print("1. å®‰è£… AKShare: pip install akshare")
    print("2. å®ç°çœŸå®PDFä¸‹è½½é€»è¾‘")
    print("3. æ¥å…¥AIæ¨¡å‹ç”Ÿæˆæ’­å®¢è„šæœ¬")
    print("4. é›†æˆTTSç”ŸæˆéŸ³é¢‘")
